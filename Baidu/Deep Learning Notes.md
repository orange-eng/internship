# Deep Learning Notes

**构建机器学习算法**
都是简单的秘方： 数据集、代价函数、优化过程和模型


**Dropout**
- 虽然Dropout在特定模型上每一步的代价是微不足道的，但在一个完整的系统上使用Dropout的代价可能非常显著。因为Dropout是一个正则化技术，它减少了模型的有效容量。为了抵消这种影响，我们必须增大模型规模。不出意外的话，使用Dropout时最佳验证集的误差会低很多，但这是以更大的模型和更多训练算法的迭代次数为代价换来的。对于非常大的数据集，正则化带来的泛化误差减少得很小。在这些情况下，使用Dropout和更大模型的计算代价可能超过正则化带来的好处。
- 只有极少的训练样本可用时，Dropout不会很有效。在只有不到 5000 的样本的Alternative Splicing数据集上 (Xiong et al., 2011)，贝叶斯神经网络 (Neal, 1996比Dropout表现得更好
- Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征

**对抗样本与对抗训练**
- 对抗样本，就是会使得机器学习的算法产生误判的样本
- 对抗训练，通过在原有的模型训练过程中注入对抗样本，提升模型对微小扰动的鲁棒性
