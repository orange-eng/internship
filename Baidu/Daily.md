# my daily


## 2021/7/1

今天百度实习的组会。一位是语言学背景的，介绍了语言模型linguistic model。在英语中存在很多语法现象，比如主谓一致等等，但是在机器看来，如果有时候定语从句比较长，那么干扰性极强，准确率很低。

易错的语言规则主要有 subject-verb agreement, negative polarity item, garden-path sentences, center-embedding。好文章还有：LSTMs and syntax dependency等等。

另外，对SG score delta没有很理解。

第二位，介绍了有关模型可解释性的问题。比如一个二分类问题，其实就是两种 选择，因此比较容易解释。如果一个很深的网络，那么其实很难解释。

在CV界，如何很好地解释图像，其实可以使用语义分割来形容。一个数字“2”，在机器看来如何去理解？其实就是关注几个比较重要的关键点即可。使用可视化工具来展示可解释性，会把一些集中区域进行标注。但是，这里有一个小问题：Dark knowledge指的是机器的理解思路，这个思路也许并不等于人类的理解思路。因此出现了分歧。

可是，“可解释性”如何衡量？实际上非常主观吧。确实是这样，在很多审稿人看来，就认为，和人类的认识方法一致，才算是合理。

第一次组会，看着大家进行激烈的讨论，真的可以学到很多东西鸭。这才是研究院的氛围，要学会融入。

# 2021/7/2

目的是让物理引擎学习到内部的物理逻辑，不仅仅是“看起来像”这个意思。  
如果对于一个情景而言， 为什么总是把水、沙子放到一个固定的容器里面呢？如果将容器壁删除，又会发生什么呢？这个就可以衡量出，学习是否仅仅停留在可视化？还是真正的物理性质上？

另外，每一个物体的物理性质，比如密度分布，质量等等，对物理引擎的影响又是怎样的？

就以动量守恒定理为例，我们可能已经提前接触过动量守恒，知道这个公式。但是如何让物理引擎学习到这个公式？仅仅是喂给input和output，是否可以达到这种效果！

今天早上，看到老师在跟美国的研究人员一起开组会，一口流利的英语让我十分震撼。真的好羡慕这种研究院， 经常有一些国际性的交流机会。

整个场馆，经常会听到很多人的交流声，而且非常大。很多英语，大家讨论十分具有激情。

另外，老师来得非常早，7:30准时坐在自己的位置上。今早他在开会，我正好全都听到了。身边全是国外的博士，OMG!
