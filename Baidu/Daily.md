# my daily


## 2021/7/1

今天百度实习的组会。一位是语言学背景的，介绍了语言模型linguistic model。在英语中存在很多语法现象，比如主谓一致等等，但是在机器看来，如果有时候定语从句比较长，那么干扰性极强，准确率很低。

易错的语言规则主要有 subject-verb agreement, negative polarity item, garden-path sentences, center-embedding。好文章还有：LSTMs and syntax dependency等等。

另外，对SG score delta没有很理解。

第二位，介绍了有关模型可解释性的问题。比如一个二分类问题，其实就是两种 选择，因此比较容易解释。如果一个很深的网络，那么其实很难解释。

在CV界，如何很好地解释图像，其实可以使用语义分割来形容。一个数字“2”，在机器看来如何去理解？其实就是关注几个比较重要的关键点即可。使用可视化工具来展示可解释性，会把一些集中区域进行标注。但是，这里有一个小问题：Dark knowledge指的是机器的理解思路，这个思路也许并不等于人类的理解思路。因此出现了分歧。

可是，“可解释性”如何衡量？实际上非常主观吧。确实是这样，在很多审稿人看来，就认为，和人类的认识方法一致，才算是合理。

第一次组会，看着大家进行激烈的讨论，真的可以学到很多东西鸭。这才是研究院的氛围，要学会融入。

# 2021/7/2

目的是让物理引擎学习到内部的物理逻辑，不仅仅是“看起来像”这个意思。  
如果对于一个情景而言， 为什么总是把水、沙子放到一个固定的容器里面呢？如果将容器壁删除，又会发生什么呢？这个就可以衡量出，学习是否仅仅停留在可视化？还是真正的物理性质上？

另外，每一个物体的物理性质，比如密度分布，质量等等，对物理引擎的影响又是怎样的？

就以动量守恒定理为例，我们可能已经提前接触过动量守恒，知道这个公式。但是如何让物理引擎学习到这个公式？仅仅是喂给input和output，是否可以达到这种效果！

今天早上，看到老师在跟美国的研究人员一起开组会，一口流利的英语让我十分震撼。真的好羡慕这种研究院， 经常有一些国际性的交流机会。

整个场馆，经常会听到很多人的交流声，而且非常大。很多英语，大家讨论十分具有激情。

另外，老师来得非常早，7:30准时坐在自己的位置上。今早他在开会，我正好全都听到了。身边全是国外的博士，OMG!

# 2021/7/4

今天去了一次圆明园，走了一圈，我整个人都不好了。支离破碎的大水法，到处都是灰白色的废墟，就像一群骷髅一样。我也不知道，为什么有的游客还能笑出来。  
看到很多小学生来这里参观，身上穿着大红色的短袖，很显眼。北京的这种基础教育和教育资源真的很好，好羡慕。这就意味着每个人的眼界不同。  
另外，今天约到一个健身教练，周四好好练练胸肌吧~

# 2021/7/6

晚上9点左右，孙老师忽然叫我做一个报告，关于最近做的工作。在周四，时间比较着急，而且自己才刚来一周时间，还没有什么基础。就这样，硬着头皮上

# 2021/7/8

上午10点半开始会议，一上场就是一个印度小哥，英语说得很流利，但是自带口音。  
到我了，虽然之前准备了稿子，但是临场发挥依然有些不太适应。中间还因为音频的问题，退出会议重新进来了一次。
问我问题的竟然是李平老师，这么牛的人。可能因为我听力不是很好，也没太听懂问题，就忙着回答了，导致了很多尴尬局面。还好我导师出面帮我解释了很多东西。感觉自己有点丢人，但是这种学术交流的氛围正是我想要的。真的要出国交流一段时间，才能有这么流利的口语，这么牛的学术资产。真的好羡慕身边的博士。  
好好总结一下这次答辩的经验吧：
- 1.音频出现问题之后，听清楚会议上的人是否给出了提示。把微信和入流设置成“有显示提示”模式，这样不至于说大家发现没音了，但是只有自己没发现。
- 2.当有老师提问时，可以先重复一下老师的问题，跟对方进行一个确认，然后再回答。这是一个好方法，最起码不会答非所问。感觉自己真的像一个小白，什么大场面都没经历过，真的要多向身边的博士生学习学习。
- 3.好好训练英语口语和听力吧。有时候也听不懂在问什么，然后自己也表达不清想法，两边都不知道在说什么。
- 4.制作PPT，命名格式要注意。第一次上传就被李平老师发现了。
- 5.一周有四次汇报，所以这种学术交流的模式很棒呀，新思想的传播会非常快。
- 6.第一次训练，学到的东西很多，慢慢进步就好。
- 7.另外，自己还没做出一些显著的成果，所以汇报的时候很难有说服力。
- 8.敬佩李平老师的平易近人，好厉害。说不定可以拿到李老师的推荐信呢。


晚上跟着健身教练，训练了一波胸肌操作。原来只是一根杠就可以练出结实的胸肌。因为我发现，即便是一个很小的重量，做多了之后，依然会很累，而且肌肉也能够被激发充分，这是今天学到最重要的内容。此外，学了几个新动作，而且确实对肌肉的刺激有了更加深刻的理解。争取在这两个月的时间内，把胸肌和腹肌都练出来，这样的话，开学之后也有装逼的资本，哈哈哈。


# 2021/7/9

上午10点半组会分享。董师兄讲解有关对抗学习应用于底层视觉的任务。顺便科普了一下关于何大神在图像分类上的对抗学习。我在做超分辨率的过程中，使用的SRGAN感觉并不是一种自监督，因为其实也用到了标签数据。有时间要跟董师兄好好探讨一下。

另外，在小组展示的过程中，注意一些用词，不要让大家感觉到可笑，不知道就是不知道，实事求是是最好的。

# 2021/7/10

上午去了颐和园。遇到了一个很有趣的导游，他说：颐和园三分看七分听，颐和园美不美，全凭导游一张嘴。确实如此，一进来感觉也没有什么特点，真的很像杭州西湖。前面最大的是昆明湖，旁边是万寿山，是乾隆皇帝给母亲过60大寿完成的，还动用了军费，真的很败家。
人特别多，景色也还算美，适合养老。背景有四大景点一定要去看：颐和园、故宫、长城、东龙府。有机会慢慢逛吧，开开眼界也是好事情。  
最复杂的要算是佛香阁那里，也正是颐和园最标志的建筑。依山傍水的样子超级美。里面祭祀的正是千手观音，说实话还是有点普通。这么大的地方居然是用来给人过寿的，实在是大材小用呀。慈禧年幼的时候会唱戏，就凭借着唱戏的技能，入宫最终垂帘听政。

这里还有一些跟溥仪相关的故事，小时候曾经在这里玩耍。然后周恩来接见一些国外来客也会前来颐和园。

昆明湖中很多船，周围也是人山人海。十七孔桥雪白，中间一座岛。整体感觉还是很不错，爬了爬山，练一练体能。

那个导游说的一句话挺对：来了景点瞎拍照，回去一问，啥都不知道。说的正是很多普通游客，不懂这些景点的背景知识，就走马观花，确实不值哈。

苏州街，仿照江南水乡建成，确实很像哈。

# 2021/7/13
## Meeting  
- Maximum Likelihood approach
- Expectation Maximization Algorithm and Monte Carlo variant  
- Two-Time-Scale Stochastic EM(TTSEM)
- Gaussian Mixture Models
- Deformable Template for Image Analysis

## DRMI  
Need many queries. Detection, Attack & Accuracy.  
We need Dataset of similiar distribution  
- High mutual information means high redundancy
- Goal of DRMI

## Testing OIA's linguistic abilities
- Yuhan Zhang
- How do we evaluate language models?
- What do we know from the evaluation results?
- Can SOTA models learn features unique to language?
- OIA: Open Information Annotation is a set of rule based guidelines. Base: Universal Dependency
  - Center-enbedding
  - Main verb/Reduced relative clause garden-path sentences
  - Subject/object garden-path sentences
  - Pseudo-cleft sentences
  - Pilot testing
- Next step
  - testing  suits preparations
  - Running tests in OIA/OIE models/Parsing trees
  - Evaluate and improvement

这位师姐是来自牛津大学的语言学专业，这英语真的很无敌，好羡慕。比美国本土有一拼。

# 2021/7/15
两个语言学的师姐汇报，讲真，听不太懂楼。

# 2021/7/17

## 首都博物馆
第一次见到首都博物馆是在MV“北京欢迎您”上，来到现场，果然名不虚传。虽然看似很大，其实里面还是很空，主要有东西两个展厅，6层左右。一开始以为5点就关闭，后来才知道8点。

- 一楼是夏商西周、秦汉时期。最显眼的是一辆巨大的马车，还有几个兵马俑。感觉跟照片里的还是不太一样，现场就是很震撼。
- 二楼的一条长廊里面，陈列了各种古代使用的器皿，虽然有很多裂痕，但是依然完好。每个朝代都拥有自己独特的风格。
- 三楼四楼就显得有些近现代的感觉。记得有一块又红又专的墙壁，“建党100周年”超级好看。主要有一些标志性事件的文物和书法等等。
- 五楼六楼陈列了非常多的玉器、佛像、瓷器等等，路上跟到一个导游，讲解瓷器的，十分细致。青花瓷，分为很多层，仔细一看，每一种花的风格都不一样。甚至有考古学家寻找这些花的出处，甚至可以追溯到《山海经》里面。据说一个青花瓷的价格够买一个四合院了，哈哈。
- 还有一个展厅是介绍老北京风俗的，很有特点。
- 东边的展厅是一种螺旋式的结构，很多青铜鼎、兵器、器皿。来到这里才知道，真正的镇馆之宝，都是单独存放的，所以来到博物馆多去欣赏那些具有独立包间的文物，最有价值。听说有的玉器是一亿年前的，因为人们很早就喜欢与神灵交流，玉器就成了必需品。

# 2021/7/18

## 国家博物馆
九点多来到，长安街真的很宽敞，天安门广场上人山人海。  
国家博物馆大概是首都博物馆的四倍。第一眼看上去还以为是人民大会堂，哈哈。大厅超级大，其中“古代中国”部分在负一层，“近代中国”在二层。从石器时代一直到清朝，每一个朝代都拥有自己独立的展厅，里面陈列着各种奇珍异宝，比首都博物馆要更加充裕。像四羊方尊、三星堆遗址等等，都在这里见到了原型。

很难想象，那些保存了上万年的东西可以流传至今，它们记录了地球的年龄，它们是真正的历史。走在这里，穿越时空，感受历史，以史为鉴。
# 2021/7/19
开始使用paddle重构图网络了。好好吃饭，好好休息，高效工作！

# 2021/7/20  

## Design a better optimizer
- Escape Saddle Points: Adam > Momentum > SGD
- Select Flat Minima: Momentum = SGD > Adam
- Adaptive Inertia Optimizer(Adai)
- Neural Variable Risk
- Neural Variability in Neuroscience
- Noise memorization and overfitting

**Neural Variability is very helpful for DL**
**NVRM can effictively relieve two serious issues: overfitting and catastrophic frogetting**
**We can theoretically understand NVRM from the information theoretical perspective and the diffusion perspective**

##Future Work
**Manipulating Stochastic Gradient Noise**
**Understanding and Scheduling Weight Decay**

印度研究员的口语也很棒呀！ How do you compare your way with other.

## Open Commonsense Reasoning with pen Information Annotation Graphs
- Training Dense Passage Retriever
- Inference with Dense Passage Retriever

- Encode Single OIA Graphs
- Encode Joint OIA-Graphs
- Subgraph Reasoning on Joint-OIA Graph

They will offer some useful suggestions for your current works.

# 2021/7/21

今天，感冒了。状态不是很好。虽然感冒不是很严重，但是注意力无法集中。  
放眼望去，大家都在看论文，改代码等等，这种氛围正是我想要的呀。

# 2021/7/23
## Graph Generation with Reinforcement Learning
- Sequence to Graph:
  - Sentence to Dependency Graph
  - Sentence to University Dependency
- Challenges
  - Structures and sizes of graphs are different
  - No orders between nodes
  - Discrete
  - Non-Euclidean
  - Generation Efficiency
- OIE Graph
  - Multi Sampling
  - Generate multiple candidate Graphs
  - Combine with beam search
- Supervised Method
  - Features to Adjacency Metrics Classication
- Generative Models
  - case study: MolGAN
  - as MDP
- Reinforement Learning for OIE Graph
  - Actor-Critic
  - Policy Gradient
  - Update Model
  - Data Collection
  - Monte Carlo tree search:
  - Selection->Expansion->Simulation->Backpropagation->Selection
- Experiment
  - Initial Results for OIE Graph Generation Task
- Next Steps
  - Improve RL Performance
  - Complete Monte Carlo Implementation
  - Complete multi sampling components

## University of Brith Columbia
- first-order methods for structured optimization
- Optimization, machine learning, data mining
  - General purpose optimization solver
  - Training neural networks
  - Training classic machine learning models
  - Signal processing
  - Discrete optimization
  - PhD study

- Motivation
  - In the practice of training machine learning models
  - Nonsmoothness from the model does not slow down our training in practice
  - The learning rate scheduling can yield
- Contributions
  - We obtain ... for stochastic subgradient descent inder convex and interpolation assumption
  - iteration complexity under strong convexity
- Assumptions
  - immediate question: rate for absolute value function?
  - we assume the 1-dimensional loss function to be nonnegative, convex, 1-smooth function and inf int=0
- Some semismooth properties cont
- Convergence under convexity
- Convergence under strong convexity
- Lower bound
  - with/without interpolation



## 面试：
- 1.聊一聊那个实习的经历，自己的贡献吧
- 2.谈一谈research plan吧
- 3.有没有一些其他的想法呢?   
- 4.具体的研究课题有没有呢？  
发现跟博士聊天就是不一样，怎么说，很难说清楚。偏理论的合作？面试官很亲和哦。不会很push你。


## 聚餐
有幸跟CCL的各位同事一起吃饭，实在是太荣幸了。没想到在这里也能遇到中科大的校友，正好老师是我师兄，哈哈。距离瞬间拉近了。老师是从中科大学数学出身，果然基础学科就是吃香，中科大校友真的随处可见。  
虽然老师们都很有资历，但是聊天起来却十分亲和，各种开玩笑，氛围超级好。有时候也会有一些滑稽的语言。能和大家一起讨论学术，一起学习，一起科研，这种环境真的求之不得。  
还想说一句，北京的物价真的好贵哦，简单一道白菜就要60+，这个账单分分钟上千。不敢想，北京的海底捞居然贵成这个样子。

受到了老师的Push,很着急，因为也想做出点成绩。等代码复原完了，开始跑实验了，或许就好很多。生成数据已经解决了，最重要的就是把数据转化成想要的格式，然后输入到网络中即可开始训练了，然后就可以有数据出来。

# 2021/7/25
今天游览的故宫博物院和天坛公园，因为路上跟着几个导游，所以还是可以学到很多东西。  
先说说故宫吧。从午门开始就是入口，一进来先看到保和门，是上朝的地方，传说康熙打天下，雍正保天下，乾隆败天下，乾隆皇帝原来也是个败家子。每次上朝，大臣都要起早，为了偷懒，建议皇上把上朝的时间推迟，但是康熙说移动到后面的那一扇门上朝，所谓的“御门听政”由此而来。之后就是三大殿，太和殿和中和殿，保和殿，中间的那座是最小的，是用来让皇帝休息，准备去太和殿上朝的。电视上最常看到的就是太和殿，最大的，最显眼的。  
往后走就是乾清门，御门听政就是在这里了。乾坤表示天地，皇上表示乾，皇后表示坤，往后走又是三大殿：乾清宫、交泰殿、坤宁宫。比之前的小了一点。  
传说这里有很多金色的大缸，八国联军入侵的时候想搬走这些，但是搬不动，于是开始用刀划，发现里面是青铜色（这是和珅大贪官的杰作），还可以看到很多划痕。  
再往后还有御花园，连理树很显眼。周边的很多宫殿确实长得很像，还是要跟着导游。有一些北京胡同，好传统，很有感觉。  

天坛。最想看的那便是祈年殿，也是天坛的标志性建筑。天坛是用来皇帝和上天沟通的。使用绿色的琉璃瓦片，代表皇帝向天称臣。祈年殿里面有四个最粗的柱子代表四季，周围又有12个柱子代表12个时辰。旁边有一个斋地，是祭奠上天之前需要先斋戒几天。  
天台是由白色的玉石构成，可以上去看。很壮观。

开眼界，开心~

# 2021/7/27

## Noise-robust information aggregation
- Generate single-doc encoder
- Accumulate knowledge
- Select the most representative title of an article to be the headline of the story
- Distant Supervion: Estimate Title Representative

## UCPhrase: Unsupervised for phrase mining
- Challenges for phrase mining
  - Models
    - Statistics-based Models
    - Tagging-based Models
  - Supervision
    - Human Annotation

- Step1:free supervision directly from the corpus
  - How do human readers accumulate phrases?
  - Mining core phrases as silver labels
  - Compare core phrases with distant Supervision

- Step2: suface-agnostic feature generation
  - What's wrong with traditional embedding-based features?
  - Extract knowledge directly from a pre-trained language model

- Step3: phrase tagging as image classification
  - Given a sentence, treat all possible ngrams as candidates

## Adversarially Robust ML

### Towards Robust Detection of Adversarial Examples

- Phase1: reverse training
  - Feature are mapped onto low-dimensional manifolds
- Property1: consistent and unbiased
- Property2: tighter bound
- Intuitive insights of RCE training

- robustness requires more data
- Our solution: increase sample density to induce locally sufficient by softmax cross-entropy loss
- Our method: Max-Mahalanobis center(MMC) loss

### Max-Mahalanobis Linear Discriminant Analysis Networks

- What if we could compute T-Con during inference?

# 2021/7/29

## Backdoor Attack
指的是在正常样本上，测试成功，但是在被后门激活之后，测试识别失败。  
比较典型的说法有： Adversarial Example,  Data poison, Backdoor attack,发生在Data Collection中  
- 后门攻击主要发生在：
  - 公开数据集
  - 预训练模型
- 评价指标为 Attack success rate

- Abnormal Detection
- Back translation
- Noise disturb
- Design unnoticeable backdoor trigger
- Defense

## Discourse, sentential information structure
消除歧义， Linguistic Discourse Model, basic discourse units  
coordination and subcoordination  
theme & rheme, background & focus

# 2021/7/30
## Arbitrary-order Proximity Preserved Network Embedding
- Different High-Order Proximities
  - Different networks/tasks require different high-order Proximities
  - Proximities of different orders can be arbitrarily weighted
  - Eigen-decomposition reweighting
  - Shifting across different orders/weights
- Network Embedding in Dynamic Environment
  - RandNE: Iterative Projection

- Can GNNs Fully Preserve Graph Structures?
- Eigen-GNN: A Graph Structure Preserving Plug-in

- Existing Problems in Traditional Graph Learning methods
   - Manually design architectures and hyper-parameters through trial-and-error
   - Each task needs be handled separately
- Automated Graph learning
  - AutoGL Dataset: manage graph datasets
  - AutoGL Solver: a high-level API to control the overall pipeline
  - Hyper-Parameter

## Multivariate Features
- Many phenomena emerge from interactions among agents
- Measuring Uncertainty: Entropy
- Mutual Infformation is Non-Linear
- Information Decomposition Through Projections
- Application: Hypothesis
- Application: Interactions in Brain Activities
### Gaussian Channel models

#2021/7/31
今天上午去了长城。6点多就起床了，赶着最早的S2，去了八达岭长城。目前为止，所有最著名的景点都逛过了。  
说实话，长城给人的感觉一直都是气势恢宏，来到现场，人山人海，但长城依然像一条巨龙，盘旋在山林中。对了，在乘坐S2的途中，还意外欣赏了京张铁路的“人字形”结构，有趣哦。  
长城分为北长城和南长城，最高的地方在北八楼。正是夏天，山林郁郁葱葱，拍了很多张照片，很开心哦。  
感觉北京的几个著名景点都已经参观过了，想想接下来的几个周末应该如何安排呢？  
真三国无双也玩完了，解锁吕布之后，整个游戏也没有什么乐趣，也算完成了儿时的心愿，没有遗憾。  
X战警系列也追完了。一开始几部是基于老的X教授和万磁王之间的情感纠纷，中间有一个《逆转未来》直接上演了新的一批演员，之间的关系更加复杂。核心不变的矛盾是变种人和正常人之间的隔阂，贯穿整个电影始终。这也确实让我们去思考，到底是应该拥有超能力，还是排斥。  
X战警中存在的超级英雄实在太多，这也意味着很多人物的性格特点很难去塑造。  
这里提到一部，暮年金刚狼，一反常态，并没有太多超级英雄的出场，通过情怀来将金刚狼的人物形象表现地淋漓尽致。画风惊奇，有创新。  
最后提一句，今天看了一个TED演讲，有一句话很有收获：你的空闲时间，决定了你是一个怎样的人。

# 2021/8/3
## Nonlinear Eigenvalue Problem: general picture
- Linearization
- Eigenvector Dependent Nonlinear Eigenvalue Problems  
Comments: Some of eigenvectors of the Linear and LargeAE nets show remarkable similarity with digits.

Pros and Cons  
- the meaning ofeigenvalue for neural networks
- Theory limited

## G
The factor analysis can be generalized to independent component analysis(ICA), sparse coding  
- Deep Latent Variable Model
  - Nonlinear mapping by neural network
  - MCMC Inference
  - variational inference
  - Adversarial learning

- Latent Space Flow-Based Prior Model
  - analytically tractable normalized density
  - easy to draw samples from by using ancestral sampling
- Generative Learning of Flow-Based Prior Model
  - Learning prior model
  - Learning generation model

# 2021/8/5
## 越高贵的人，越谦卑
- 在这里，我发现蔡老师就是这句话的真实写照。带人亲和友善，喜欢开玩笑，非常有趣的一位老师。学数学的老师就是厉害，如果可以像他一样就好了。
- 今天Robomaster比赛结束，华南虎排名季军。赛程非常紧迫，两三天就比完，大家的付出都有所回报，就已经心满意足，继续加油吧，少年~

## meeting
### Rule mining
- Argument sky -> cloud
- Predicate and one of its Argument

- 2 steps: Entity Typing -> Relation Typing
- 1 step: Fact about type

**Configuration**
- one transaction
- triple like r(a,b)
**AMIE**
- Sampling: Find entity chains from two
- Predicate selection: Synonymy scoring, Co-occurence scoring
**Rule Evaluation**
- Foreseeable diifculty : Document level transaction, knowledge-base

- Questions
  - 关联规则可挖掘。推理用“蕴含关系”来表示
  - 因果关系和蕴含关系是币一样的
  - 可读那么一定可写，但是可写不一定可读
  - baseline是否可行，能挖出来什么？

- Tasks
  - 使用paddle还原算法之后，观察是否真的可以学到原有的性质
  - 时间和相似度有怎样的提升效果
  - LBM的对比验证
  - 精度是否有提高
  - 可以学习到多少的interaction
  - 与传统的物理引擎相比，基于学习的物理引擎在于，无需约束条件


# 2021/8/6
## DE-GNN
- comprehensive representation of scene graphs by encoding graphs' object-significant modal
- Test on GQA & VQA-V2 dataset
- explicit modeling of attributes impacts Performance
